# reproducibility
seed_index: 0
deterministic: False

# PyTorch Lightning Trainer https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html
trainer:
  fast_dev_run: False # Enable this for debug purposes
  gpus: 1
  precision: 32
  max_epochs: null
  max_steps: 100000
  accumulate_grad_batches: null
  num_sanity_val_steps: 2
  gradient_clip_val: 10.0
  val_check_interval: 10000
  deterministic: ${train.deterministic}
  ## Aumentare max_steps (in futuro eliminare grad_clip_val)

restore:
  ckpt_or_run_path: /home/molfese/data/neural-sentence-aligner/storage/storage/sentence_aligner/2iz2jl7f/checkpoints/epoch=0-step=1249999.ckpt.zip
  mode: finetune # null, finetune, hotstart, continue

monitor:
  metric: 'val_f1_full'
  mode: 'max'

callbacks:
  - _target_: pytorch_lightning.callbacks.EarlyStopping
    patience: 10
    verbose: False
    monitor: ${train.monitor.metric}
    mode: ${train.monitor.mode}

  - _target_: pytorch_lightning.callbacks.ModelCheckpoint
    save_top_k: 1
    verbose: False
    monitor: ${train.monitor.metric}
    mode: ${train.monitor.mode}

  - _target_: pytorch_lightning.callbacks.LearningRateMonitor
    logging_interval: "step"
    log_momentum: False

  - _target_: pytorch_lightning.callbacks.progress.tqdm_progress.TQDMProgressBar
    refresh_rate: 20

logging:
  upload:
    run_files: true
    source: true

  logger:
    _target_: pytorch_lightning.loggers.WandbLogger

    project: ${core.project_name}
    log_model: False
    mode: 'online'
    tags: ${core.tags}

  wandb_watch:
    log: 'all'
    log_freq: 100
