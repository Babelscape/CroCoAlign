transformer_name: sentence-transformers/LaBSE
tokenizer_transformer_name: sentence-transformers/LaBSE
data:
  _target_: sentence_aligner.data.datamodule.MyDataModule
  datasets:
    train:
      _target_: sentence_aligner.data.dataset.MyDataset
      split: train
    val:
      _target_: sentence_aligner.data.dataset.MyDataset
      split: val
    test:
      _target_: sentence_aligner.data.dataset.MyDataset
      split: test

  precomputed_embeddings: False
  gpus: ${train.trainer.gpus}
  root_path: ${core.data_dir}
  num_workers:
    train: 1
    val: 1
    test: 1

  batch_size:
    train: 64
    val: ${nn.data.batch_size.train}
    test: ${nn.data.batch_size.train}

  transformer_name: ${nn.transformer_name}
  max_ids: 2000
  
  train_dataset_names: null
  val_dataset_names: null
  test_dataset_names: null

module:
  _target_: sentence_aligner.pl_modules.pl_module.MyLightningModule
  transformer_name: ${nn.transformer_name}
  warmup_percentage: 0.1
  freeze_encoder: True
  precomputed_embeddings: False
  dropout: 0.2
  sentence_context_n_heads: 6
  sentence_context_n_layers: 6
  optimizer:
    #  Adam-oriented deep learning
    _target_: torch.optim.AdamW
    #  These are all default parameters for the Adam optimizer
    lr: 5e-06
    betas: [ 0.9, 0.999 ]
    eps: 1e-08
    weight_decay: 0.01

  lr_scheduler:
    _target_: transformers.get_linear_schedule_with_warmup
    num_training_steps: ${train.trainer.max_steps}
