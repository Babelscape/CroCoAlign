{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/molfese/miniconda3/envs/sentence-aligner/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np ## pip install numpy\n",
    "import faiss ## conda install -c conda-forge faiss-gpu\n",
    "import torch ## conda install pytorch torchvision torchaudio pytorch-cuda=11.6 -c pytorch -c nvidia (Select the right CUDA version for your GPU).\n",
    "import torch.nn as nn\n",
    "from sentence_transformers import SentenceTransformer ## pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sentence encoder model. Other models can be found here: https://huggingface.co/sentence-transformers.\n",
    "# The first time you load a sentence encoder, it will be downloaded and saved in the .chace folder for future reuse. \n",
    "transformer_name = \"sentence-transformers/LaBSE\"\n",
    "sentence_encoder = SentenceTransformer(transformer_name).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random source and target sentences. (Punctuation matters a lot unfortunately).\n",
    "sentence = \"Today is a beautiful day!\"\n",
    "wordnet_sentences = [\n",
    "    \"Today is a good day.\", \n",
    "    \"Yesterday was a good day.\",\n",
    "    \"Today is a good day!\", ## This should be the best according to meaning and punctuation.\n",
    "    \"Yesterday was a good day!\", \n",
    "    \"Today is a bad day.\", \n",
    "    \"Today is a bad day!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of source and target embeddings through the sentence encoder.\n",
    "sources_emb_list = []\n",
    "targets_emb_list = []\n",
    "source_emb = sentence_encoder.encode(sentence)\n",
    "target_embs = sentence_encoder.encode(wordnet_sentences)\n",
    "sources_emb_list.append(source_emb)\n",
    "targets_emb_list.extend(target_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 768), (6, 768))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert everything into a matrix as required by FAISS.\n",
    "sources_emb_matrix = np.array(sources_emb_list, dtype=np.float32)\n",
    "targets_emb_matrix = np.array(targets_emb_list, dtype=np.float32)\n",
    "sources_emb_matrix.shape, targets_emb_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.95209295, 0.88073856, 0.7944416 ]], dtype=float32),\n",
       " array([[2, 5, 3]]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 3 # Number of candidates.\n",
    "# Now we build the FAISS index to load source and target matrices.\n",
    "index = faiss.index_factory(sources_emb_matrix.shape[1], \"Flat\", faiss.METRIC_INNER_PRODUCT) # METRIC_INNER_PRODUCT = Similarity metric between vectors.\n",
    "faiss.normalize_L2(sources_emb_matrix) # Normalize data for efficiency.\n",
    "faiss.normalize_L2(targets_emb_matrix) # Normalize data for efficiency.\n",
    "index.add(targets_emb_matrix) \n",
    "\n",
    "D, I = index.search(sources_emb_matrix[0:1], k) # Search for the K most similar sentences among the wordnet_sentences.\n",
    "# D = array of the distances between source and best K candidates (based on the chosen similarity metric above).\n",
    "# I = array of the indexes of the best K candidates (based on the chosen similarity metric above).\n",
    "D, I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.95209295, 0.88073856, 0.7944416 ], dtype=float32), array([2, 5, 3]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: FAISS, in general, is developed to do this procedure for N source sentences, therefore it will return D and I as matrices. \n",
    "# Since we have only one source sentence, we need to access the first (0) element of D and I.\n",
    "D[0], I[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today is a good day!', 'Today is a bad day!', 'Yesterday was a good day!']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's retrieve the best candidates (already ordered by similarity).\n",
    "best_candidates = [wordnet_sentences[i] for i in I[0]]\n",
    "best_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import json\n",
    "import os\n",
    "\n",
    "train_dir = \"../../../data-without-embeddings/opus/books/train/\"\n",
    "val_dir = \"../../../data-without-embeddings/opus/books/val/\"\n",
    "test_dir = \"../../../data-without-embeddings/opus/books/test/\"\n",
    "train_dict = {}\n",
    "val_dict = {}\n",
    "test_dict = {}\n",
    "for file in os.listdir(train_dir):\n",
    "    with jsonlines.open(train_dir + file) as reader:\n",
    "        for obj in reader:\n",
    "            sl = len(obj[\"sources\"][\"ids\"]) if obj[\"sources\"][\"ids\"] != [\"\"] else 0\n",
    "            tl = len(obj[\"targets\"][\"ids\"]) if obj[\"targets\"][\"ids\"] != [\"\"] else 0\n",
    "            if (sl, tl) not in train_dict:\n",
    "                train_dict[(sl, tl)] = 1\n",
    "            else:\n",
    "                train_dict[(sl, tl)] += 1\n",
    "for file in os.listdir(val_dir):\n",
    "    with jsonlines.open(val_dir + file) as reader:\n",
    "        for obj in reader:\n",
    "            sl = len(obj[\"sources\"][\"ids\"]) if obj[\"sources\"][\"ids\"] != [\"\"] else 0\n",
    "            tl = len(obj[\"targets\"][\"ids\"]) if obj[\"targets\"][\"ids\"] != [\"\"] else 0\n",
    "            if (sl, tl) not in val_dict:\n",
    "                val_dict[(sl, tl)] = 1\n",
    "            else:\n",
    "                val_dict[(sl, tl)] += 1\n",
    "for file in os.listdir(test_dir):\n",
    "    with jsonlines.open(test_dir + file) as reader:\n",
    "        for obj in reader:\n",
    "            sl = len(obj[\"sources\"][\"ids\"]) if obj[\"sources\"][\"ids\"] != [\"\"] else 0\n",
    "            tl = len(obj[\"targets\"][\"ids\"]) if obj[\"targets\"][\"ids\"] != [\"\"] else 0\n",
    "            if (sl, tl) not in test_dict:\n",
    "                test_dict[(sl, tl)] = 1\n",
    "            else:\n",
    "                test_dict[(sl, tl)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = dict(sorted(train_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "val_dict = dict(sorted(val_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "test_dict = dict(sorted(test_dict.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"test_dict.tsv\", \"w\") as fout:\n",
    "    writer = csv.writer(fout, delimiter=\"\\t\")\n",
    "    writer.writerow([\"comb\", \"count\"])\n",
    "    for k,v in test_dict.items():\n",
    "        writer.writerow([k, v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"../../../data-without-embeddings/opus/books/train/\"\n",
    "val_dir = \"../../../data-without-embeddings/opus/books/val/\"\n",
    "test_dir = \"../../../data-without-embeddings/opus/books/test/\"\n",
    "train_len = {}\n",
    "val_len = {}\n",
    "test_len = {}\n",
    "for file in os.listdir(train_dir):\n",
    "    source_lang = file[6:8]\n",
    "    target_lang = file[9:11]\n",
    "    with jsonlines.open(train_dir + file) as reader:\n",
    "        for obj in reader:\n",
    "            sl = len(obj[\"sources\"][\"ids\"]) if obj[\"sources\"][\"ids\"] != [\"\"] else 0\n",
    "            tl = len(obj[\"targets\"][\"ids\"]) if obj[\"targets\"][\"ids\"] != [\"\"] else 0\n",
    "            if (source_lang, target_lang) not in train_len:\n",
    "                train_len[(source_lang, target_lang)] = [sl, tl]\n",
    "            else:\n",
    "                train_len[(source_lang, target_lang)][0] += sl\n",
    "                train_len[(source_lang, target_lang)][1] += tl\n",
    "for file in os.listdir(val_dir):\n",
    "    source_lang = file[4:6]\n",
    "    target_lang = file[7:9]\n",
    "    with jsonlines.open(val_dir + file) as reader:\n",
    "        for obj in reader:\n",
    "            sl = len(obj[\"sources\"][\"ids\"]) if obj[\"sources\"][\"ids\"] != [\"\"] else 0\n",
    "            tl = len(obj[\"targets\"][\"ids\"]) if obj[\"targets\"][\"ids\"] != [\"\"] else 0\n",
    "            if (source_lang, target_lang) not in val_len:\n",
    "                val_len[(source_lang, target_lang)] = [sl, tl]\n",
    "            else:\n",
    "                val_len[(source_lang, target_lang)][0] += sl\n",
    "                val_len[(source_lang, target_lang)][1] += tl\n",
    "for file in os.listdir(test_dir):\n",
    "    source_lang = file[5:7]\n",
    "    target_lang = file[8:10]\n",
    "    with jsonlines.open(test_dir + file) as reader:\n",
    "        for obj in reader:\n",
    "            sl = len(obj[\"sources\"][\"ids\"]) if obj[\"sources\"][\"ids\"] != [\"\"] else 0\n",
    "            tl = len(obj[\"targets\"][\"ids\"]) if obj[\"targets\"][\"ids\"] != [\"\"] else 0\n",
    "            if (source_lang, target_lang) not in test_len:\n",
    "                test_len[(source_lang, target_lang)] = [sl, tl]\n",
    "            else:\n",
    "                test_len[(source_lang, target_lang)][0] += sl\n",
    "                test_len[(source_lang, target_lang)][1] += tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = dict(sorted(train_len.items(), key=lambda item: item[1][0], reverse=True))\n",
    "val_len = dict(sorted(val_len.items(), key=lambda item: item[1][0], reverse=True))\n",
    "test_len = dict(sorted(test_len.items(), key=lambda item: item[1][0], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"test_len.tsv\", \"w\") as fout:\n",
    "    writer = csv.writer(fout, delimiter=\"\\t\")\n",
    "    writer.writerow([\"source\", \"target\", \"source_len\", \"target_len\"])\n",
    "    for k,v in test_len.items():\n",
    "        writer.writerow([k[0], k[1], v[0], v[1]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentence-aligner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eef114e886804cbddefc8b9abcafed098f460a529cd116f3387a6600484bde44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
